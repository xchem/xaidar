{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b8135e",
   "metadata": {},
   "source": [
    "# <p align=\"center\"> Get Object Store File Sizes </p>\n",
    "\n",
    "This notebooks shows the code used to extract the sizes the files for each object key in each object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa12c1a",
   "metadata": {},
   "source": [
    "Vibe Code - created by chatgpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "credKey = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Store Names:['XChem', 'MinIO']\n",
      "Credentials Associated with each Object Store: ['endpoint_url', 'access_key', 'secret_key']\n",
      "Started\n",
      "Fetched sizes in 6.666420936584473 seconds\n",
      "Saved Path: /home/alex/Documents/xaidar/data/s3Sizes/pandda/frag30.pkl_sizes.pkl\n",
      "Saved pandda-frag30.pkl_sizes.pkl: 6.668848276138306\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append( Path( \".\" ).resolve().__str__() )\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from xaidar.filesUtils import loadPickle, savePyObj\n",
    "from xaidar.s3Utils import decryptCredentials, initialize\n",
    "\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "credDic = decryptCredentials( credKey, Path( \"../../credentials.enc\") )\n",
    "# Initialize boto3 client\n",
    "s3 = initialize(\"XChem\", credDic)\n",
    "\n",
    "def get_object_size(bucket_name, key):\n",
    "    try:\n",
    "        response = s3.head_object(Bucket=bucket_name, Key=key)\n",
    "        return key, response['ContentLength']  # Size in bytes\n",
    "    except Exception as e:\n",
    "        return key, f\"Error: {e}\"\n",
    "\n",
    "def fetch_sizes_parallel(bucket_name, keys, max_workers=20):\n",
    "    sizes = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_key = {executor.submit(get_object_size, bucket_name, key): key for key in keys}\n",
    "        for future in as_completed(future_to_key):\n",
    "            key, size = future.result()\n",
    "            sizes[key] = size\n",
    "\n",
    "    return sizes\n",
    "for bucket_name in [\"xchem\", \"pandda\"]:\n",
    "    bucket_name = \"pandda\"\n",
    "    for frag in Path( f\"../../data/s3ObjKeys/{bucket_name}\" ).iterdir():\n",
    "        if frag.is_file():\n",
    "            keys = loadPickle( frag )[1: ]\n",
    "            start = time.time()\n",
    "            print(\"Started\")\n",
    "            sizes = fetch_sizes_parallel( bucket_name, keys, max_workers=40 )\n",
    "            print( f\"Fetched sizes in {time.time() - start} seconds\")\n",
    "\n",
    "            savePath = Path(\"../../data/s3Sizes\").joinpath( bucket_name, f\"{frag.name}_sizes.pkl\" ).resolve()\n",
    "            print( \"Saved Path: {}\".format( savePath ) )    \n",
    "            savePath.parent.mkdir( parents=True, exist_ok=True )\n",
    "            savePyObj( sizes, savePath ) \n",
    "            print( f\"Saved {bucket_name}-{frag.name}_sizes.pkl: { time.time() - start }\" )\n",
    "    #         break\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "    \n",
    "    # keys = loadPickle( Path( \"../../data/s3ObjKeys/pandda/frag1.pkl\") )[1: 1000]\n",
    "    # start = time.time()\n",
    "    # print(\"Started\")\n",
    "    # sizes = fetch_sizes_parallel( bucket_name, keys, max_workers=20 )\n",
    "    # print( f\"Fetched sizes in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a03e50",
   "metadata": {},
   "source": [
    "Exploaration: Looking at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129d4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append( Path( \".\" ).resolve().__str__() )\n",
    "\n",
    "from xaidar.filesUtils import loadPickle\n",
    "\n",
    "fragPath = Path( \"../../data/s3Sizes/xchem/frag2_sizes.pkl\" )\n",
    "frag = loadPickle( fragPath )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32da3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "10000877\n"
     ]
    }
   ],
   "source": [
    "print( type( frag) )\n",
    "print( len( frag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25962f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictKeys = list( frag.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3011e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f0306950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/2015/lb13308-1/processing/old_processing_prior_to_20160520/analysis/dimple-maps/KRASQ61/best/KRASQ61-x92_1_3d-run/ini.pdb\n",
      "731684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print( dictKeys[ idx ] )\n",
    "print( frag[ dictKeys[idx] ] )\n",
    "idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
